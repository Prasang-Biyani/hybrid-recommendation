{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPKJuc2B17Qh",
        "outputId": "e6b7fc4d-b760-481c-8dda-db0f6dfe7f90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/cosine_similarity_computation\n",
            "total 624530\n",
            "-rw------- 1 root root 125947272 Oct 31 03:06 __installer__.sh\n",
            "-rw------- 1 root root  58468498 Jun  7  2018 Miniconda3-4.5.4-Linux-x86_64.sh\n",
            "drwx------ 2 root root      4096 Oct 15 12:29 rapidsai-csp-utils\n",
            "-rw------- 1 root root  18274041 Oct 31 02:12 reg_test_15KU.csv\n",
            "-rw------- 1 root root 221531223 Oct 25 07:22 reg_train_30KU.csv\n",
            "-rw------- 1 root root    171892 Oct 31 02:55 sample_test_sparse_matrix_15KU_1.5KM.npz\n",
            "-rw------- 1 root root   2130932 Oct 19 10:28 sample_train_sparse_matrix_28KU_3.5KM.npz\n",
            "drwx------ 2 root root      4096 Oct 31 03:06 surprise\n",
            "-rw------- 1 root root  45559912 Mar 21  2018 test_sparse_matrix.npz\n",
            "-rw------- 1 root root 167424989 Mar 21  2018 train_sparse_matrix.npz\n"
          ]
        }
      ],
      "source": [
        "%cd \"drive/MyDrive/cosine_similarity_computation/\"\n",
        "!ls -al"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztqqaRKa3t9H",
        "outputId": "a721879c-0878-47ee-8011-ccb594c1da7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "***********************************************************************\n",
            "Woo! Your instance has the right kind of GPU, a Tesla V100-SXM2-16GB!\n",
            "***********************************************************************\n",
            "\n",
            "Requirement already satisfied: cupy-cuda111 in /usr/local/lib/python3.7/dist-packages (9.4.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.17 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda111) (1.19.5)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.7/dist-packages (from cupy-cuda111) (0.6)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.2 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.19.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "# !git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/env-check.py\n",
        "!pip3 install cupy-cuda111\n",
        "!pip3 install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1roJgOHx14Y",
        "outputId": "7ce3a3c5-efb8-4aa8-ccc4-9fbe9c7a1c4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Updating your Colab environment.  This will restart your kernel.  Don't Panic!\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:4 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Hit:6 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Get:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:8 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease [20.8 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:11 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Get:12 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,810 kB]\n",
            "Ign:13 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:14 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [696 B]\n",
            "Hit:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:16 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,837 kB]\n",
            "Get:18 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [927 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [667 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,213 kB]\n",
            "Get:21 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [44.7 kB]\n",
            "Get:22 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 Packages [50.4 kB]\n",
            "Get:24 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [786 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,434 kB]\n",
            "Get:26 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [633 kB]\n",
            "Get:27 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,400 kB]\n",
            "Fetched 14.1 MB in 2s (6,780 kB/s)\n",
            "Reading package lists... Done\n",
            "Added repo\n",
            "Hit:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:3 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:7 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic InRelease\n",
            "Hit:10 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:14 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Reading package lists... Done\n",
            "Installing libstdc++\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Selected version '11.1.0-1ubuntu1~18.04.1' (Toolchain test builds:18.04/bionic [amd64]) for 'libstdc++6'\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following NEW packages will be installed:\n",
            "  gcc-11-base libgcc-s1\n",
            "The following packages will be upgraded:\n",
            "  libstdc++6\n",
            "1 upgraded, 2 newly installed, 0 to remove and 74 not upgraded.\n",
            "Need to get 641 kB of archives.\n",
            "After this operation, 981 kB of additional disk space will be used.\n",
            "Get:1 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 gcc-11-base amd64 11.1.0-1ubuntu1~18.04.1 [19.0 kB]\n",
            "Get:2 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libgcc-s1 amd64 11.1.0-1ubuntu1~18.04.1 [41.8 kB]\n",
            "Get:3 http://ppa.launchpad.net/ubuntu-toolchain-r/test/ubuntu bionic/main amd64 libstdc++6 amd64 11.1.0-1ubuntu1~18.04.1 [580 kB]\n",
            "Fetched 641 kB in 0s (3,883 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package gcc-11-base:amd64.\n",
            "(Reading database ... 155062 files and directories currently installed.)\n",
            "Preparing to unpack .../gcc-11-base_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Setting up gcc-11-base:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Selecting previously unselected package libgcc-s1:amd64.\n",
            "(Reading database ... 155067 files and directories currently installed.)\n",
            "Preparing to unpack .../libgcc-s1_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Replacing files in old package libgcc1:amd64 (1:8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libgcc-s1:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "(Reading database ... 155069 files and directories currently installed.)\n",
            "Preparing to unpack .../libstdc++6_11.1.0-1ubuntu1~18.04.1_amd64.deb ...\n",
            "Unpacking libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) over (8.4.0-1ubuntu1~18.04) ...\n",
            "Setting up libstdc++6:amd64 (11.1.0-1ubuntu1~18.04.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!bash rapidsai-csp-utils/colab/update_gcc.sh\n",
        "import os\n",
        "os._exit(00)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67pTJaWW-e4r",
        "outputId": "671bd832-521c-4727-e439-b6680b6ed340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "â¬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "ðŸ“¦ Installing...\n",
            "ðŸ“Œ Adjusting configuration...\n",
            "ðŸ©¹ Patching environment...\n",
            "â² Done in 0:00:24\n",
            "ðŸ” Restarting kernel...\n"
          ]
        }
      ],
      "source": [
        "# This will install CondaColab.  This will restart your kernel one last time.  Run this cell by itself and only run the next cell once you see the session crash.\n",
        "import condacolab\n",
        "condacolab.install()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-MCHs7y1Ckg",
        "outputId": "b18721a6-9e77-4541-83a0-4b7343c1489a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ¨ðŸ°âœ¨ Everything looks OK!\n"
          ]
        }
      ],
      "source": [
        "# you can now run the rest of the cells as normal\n",
        "import condacolab\n",
        "condacolab.check()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4TxpeAD1Exp"
      },
      "outputs": [],
      "source": [
        "%cd \"drive/MyDrive/cosine_similarity_computation/\"\n",
        "\n",
        "!python3 rapidsai-csp-utils/colab/install_rapids.py stable\n",
        "import os\n",
        "os.environ['NUMBAPRO_NVVM'] = '/usr/local/cuda/nvvm/lib64/libnvvm.so'\n",
        "os.environ['NUMBAPRO_LIBDEVICE'] = '/usr/local/cuda/nvvm/libdevice/'\n",
        "os.environ['CONDA_PREFIX'] = '/usr/local'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KI_VIEQNxx9t"
      },
      "outputs": [],
      "source": [
        "import cupy as cp\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from cupyx.scipy.sparse import csr_matrix, find\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import scipy\n",
        "import xgboost as xgb\n",
        "import cuml\n",
        "import cudf\n",
        "import rmm\n",
        "from cuml.metrics.pairwise_distances import pairwise_distances, sparse_pairwise_distances\n",
        "\n",
        "from datetime import datetime\n",
        "import os\n",
        "import random\n",
        "\n",
        "cp.cuda.set_allocator(rmm.rmm_cupy_allocator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CF6NU3kx-oVn"
      },
      "outputs": [],
      "source": [
        "train_sparse = csr_matrix(scipy.sparse.load_npz('train_sparse_matrix.npz'), dtype=cp.float32)\n",
        "test_sparse = csr_matrix(scipy.sparse.load_npz('test_sparse_matrix.npz'), dtype=cp.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6NwfYQjAYcU"
      },
      "outputs": [],
      "source": [
        "def get_sample_sparse_matrix(sparse_matrix, no_users, no_movies, path, verbose = True):\n",
        "    \"\"\"\n",
        "        It will get it from the ''path'' if it is present  or It will create \n",
        "        and store the sampled sparse matrix in the path specified.\n",
        "    \"\"\"\n",
        "\n",
        "    # get (row, col) and (rating) tuple from sparse_matrix...\n",
        "    row_ind, col_ind, ratings = find(sparse_matrix)\n",
        "    users = cp.unique(row_ind)\n",
        "    movies = cp.unique(col_ind)\n",
        "\n",
        "    print(\"Original Matrix : (users, movies) -- ({} {})\".format(len(users), len(movies)))\n",
        "    print(\"Original Matrix : Ratings -- {}\\n\".format(len(ratings)))\n",
        "\n",
        "    # It just to make sure to get same sample everytime we run this program..\n",
        "    # and pick without replacement....\n",
        "    np.random.seed(15)\n",
        "    sample_users = cp.random.choice(users, no_users, replace=False)\n",
        "    sample_movies = cp.random.choice(movies, no_movies, replace=False)\n",
        "    # get the boolean mask or these sampled_items in original row/col_inds..\n",
        "    mask = cp.logical_and( cp.isin(row_ind, sample_users),\n",
        "                      cp.isin(col_ind, sample_movies) )\n",
        "    \n",
        "    sample_sparse_matrix = csr_matrix((ratings[mask], (row_ind[mask], col_ind[mask])),\n",
        "                                             shape=(max(sample_users)+1, max(sample_movies)+1))\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Sampled Matrix : (users, movies) -- ({} {})\".format(len(sample_users), len(sample_movies)))\n",
        "        print(\"Sampled Matrix : Ratings --\", format(ratings[mask].shape[0]))\n",
        "\n",
        "    print('Saving it into disk for furthur usage..')\n",
        "    # save it into disk\n",
        "    scipy.sparse.save_npz(path, sample_sparse_matrix.get())\n",
        "    if verbose:\n",
        "            print('Done..\\n')\n",
        "    \n",
        "    return sample_sparse_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_W1LgUbAgKq",
        "outputId": "5b5d56b6-4499-4884-96a7-672e08566984"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is present in your pwd, getting it from disk....\n",
            "DONE..\n",
            "0:00:00.704481\n"
          ]
        }
      ],
      "source": [
        "start = datetime.now()\n",
        "path = \"sample_train_sparse_matrix_30KU_5KM.npz\"\n",
        "if os.path.isfile(path):\n",
        "    print(\"It is present in your pwd, getting it from disk....\")\n",
        "    # just get it from the disk instead of computing it\n",
        "    sample_train_sparse_matrix = csr_matrix(scipy.sparse.load_npz(path), dtype=cp.float32)\n",
        "    print(\"DONE..\")\n",
        "else: \n",
        "    # get 10k users and 1k movies from available data \n",
        "    sample_train_sparse_matrix = get_sample_sparse_matrix(train_sparse, no_users=30000, no_movies=5000,\n",
        "                                             path = path)\n",
        "\n",
        "print(datetime.now() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCRU81XcAru3",
        "outputId": "79f53389-0d23-4408-94d8-d6fbc52e40d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It is present in your pwd, getting it from disk....\n",
            "DONE..\n",
            "0:00:00.408112\n"
          ]
        }
      ],
      "source": [
        "start = datetime.now()\n",
        "\n",
        "path = \"sample_test_sparse_matrix_15KU_1.5KM.npz\"\n",
        "if os.path.isfile(path):\n",
        "    print(\"It is present in your pwd, getting it from disk....\")\n",
        "    # just get it from the disk instead of computing it\n",
        "    sample_test_sparse_matrix = csr_matrix(scipy.sparse.load_npz(path), dtype=cp.float32)\n",
        "    print(\"DONE..\")\n",
        "else:\n",
        "    # get 5k users and 500 movies from available data \n",
        "    sample_test_sparse_matrix = get_sample_sparse_matrix(test_sparse, no_users=15000, no_movies=1500,\n",
        "                                                 path=path)\n",
        "print(datetime.now() - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwmbK51hydEk",
        "outputId": "7e49e930-5578-4ed7-aeb2-3e51767c2707"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(cupy._core.core.ndarray, 73618)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# get users, movies and ratings from our samples train sparse matrix\n",
        "sample_train_users, sample_train_movies, sample_train_ratings = find(sample_train_sparse_matrix)\n",
        "sample_test_users, sample_test_movies, sample_test_ratings = find(sample_test_sparse_matrix)\n",
        "\n",
        "type(sample_train_users), len(sample_test_users)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssJfjHnGO2DU"
      },
      "outputs": [],
      "source": [
        "sample_train_averages = dict()\n",
        "# get the global average of ratings in our train set.\n",
        "global_average = sample_train_sparse_matrix.sum()/sample_train_sparse_matrix.count_nonzero()\n",
        "sample_train_averages['global'] = global_average\n",
        "\n",
        "# get the user averages in dictionary (key: user_id/movie_id, value: avg rating)\n",
        "\n",
        "def get_average_ratings(sparse_matrix, of_users):\n",
        "    \n",
        "    # average ratings of user/axes\n",
        "    ax = 1 if of_users else 0 # 1 - User axes,0 - Movie axes\n",
        "\n",
        "    # \".A1\" is for converting Column_Matrix to 1-D numpy array \n",
        "    sum_of_ratings = sparse_matrix.sum(axis=ax).A1\n",
        "    # Boolean matrix of ratings ( whether a user rated that movie or not)\n",
        "    is_rated = sparse_matrix!=0\n",
        "    # no of ratings that each user OR movie..\n",
        "    no_of_ratings = is_rated.sum(axis=ax).A1\n",
        "    \n",
        "    # max_user  and max_movie ids in sparse matrix \n",
        "    u,m = sparse_matrix.shape\n",
        "    # creae a dictonary of users and their average ratigns..\n",
        "    average_ratings = { i : sum_of_ratings[i]/no_of_ratings[i]\n",
        "                                 for i in range(u if of_users else m) \n",
        "                                    if no_of_ratings[i] !=0}\n",
        "\n",
        "    # return that dictionary of average ratings\n",
        "    return average_ratings\n",
        "\n",
        "sample_train_averages['user'] = get_average_ratings(sample_train_sparse_matrix.get(), of_users=True)\n",
        "sample_train_averages['movie'] = get_average_ratings(sample_train_sparse_matrix.get(), of_users=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "myHhh3AeDzZd",
        "outputId": "e56203b0-8534-46cd-9f42-242306167145"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "930000 2021244 13763\n",
            "Done for 930000 rows----- 0:26:29.036471\n",
            "940000 2046268 13418\n",
            "Done for 940000 rows----- 0:53:09.030478\n",
            "950000 2067192 6339\n",
            "Done for 950000 rows----- 1:19:26.398436\n",
            "960000 2093381 7032\n",
            "Done for 960000 rows----- 1:46:02.842839\n",
            "970000 2121294 1607\n",
            "Done for 970000 rows----- 2:12:13.299152\n",
            "980000 2146325 9946\n",
            "Done for 980000 rows----- 2:38:29.000041\n",
            "990000 2175485 14656\n",
            "Done for 990000 rows----- 3:04:30.853561\n",
            "1000000 2203332 11478\n",
            "Done for 1000000 rows----- 3:30:14.785182\n",
            "1010000 2232611 6235\n",
            "Done for 1010000 rows----- 3:56:09.441879\n",
            "1020000 2258079 16495\n",
            "Done for 1020000 rows----- 4:23:43.152927\n",
            "1030000 2284783 17506\n",
            "Done for 1030000 rows----- 4:49:30.983829\n",
            "1040000 2312320 4216\n",
            "Done for 1040000 rows----- 5:15:34.208930\n",
            "1050000 2337846 2072\n",
            "Done for 1050000 rows----- 5:41:43.429102\n",
            "1060000 2360593 2391\n",
            "Done for 1060000 rows----- 6:07:24.093991\n",
            "1070000 2392643 12125\n",
            "Done for 1070000 rows----- 6:33:08.431001\n",
            "1080000 2418149 257\n",
            "Done for 1080000 rows----- 6:59:39.913742\n",
            "1090000 2448380 9800\n",
            "Done for 1090000 rows----- 7:25:08.378189\n",
            "1100000 2480324 10241\n",
            "Done for 1100000 rows----- 7:50:53.850044\n",
            "1110000 2507316 14644\n",
            "Done for 1110000 rows----- 8:17:13.363859\n",
            "1120000 2532661 3355\n",
            "Done for 1120000 rows----- 8:43:25.971008\n",
            "1130000 2563596 17215\n",
            "Done for 1130000 rows----- 9:09:07.039831\n",
            "1140000 2595747 15569\n",
            "Done for 1140000 rows----- 9:34:43.867842\n",
            "1150000 2629127 5421\n",
            "Done for 1150000 rows----- 10:00:35.221359\n",
            "10:18:29.478044\n"
          ]
        }
      ],
      "source": [
        "############################################################\n",
        "# It took me almost 10 hours to prepare this train dataset.#\n",
        "############################################################\n",
        "start = datetime.now()\n",
        "# if os.path.isfile('reg_train_large.csv'):\n",
        "#     print(\"File already exists you don't have to prepare again...\" )\n",
        "    # print('preparing {} tuples for the dataset..\\n'.format(len(train_sparse)))\n",
        "with open('reg_train_large.csv', mode='a') as reg_data_file:\n",
        "  count = 920000\n",
        "  index = 920000\n",
        "  for _, (user, movie, rating)  in enumerate(zip(sample_train_users[920000:], sample_train_movies[920000:], sample_train_ratings[920000:])):\n",
        "        st = datetime.now()\n",
        "        user, movie, rating = int(user), int(movie), int(rating)\n",
        "        #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
        "        # compute the similar Users of the \"user\"        \n",
        "        # user_sim = cosine_similarity(sample_train_sparse_matrix[user], sample_train_sparse_matrix).ravel()\n",
        "        user_sim = 1 - pairwise_distances(sample_train_sparse_matrix[user], sample_train_sparse_matrix, metric=\"cosine\").ravel()\n",
        "        top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "        # get the ratings of most similar users for this movie\n",
        "        top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
        "        # we will make it's length \"5\" by adding movie averages to .\n",
        "        top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "        top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
        "    #     print(top_sim_users_ratings, end=\" \")    \n",
        "\n",
        "        #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
        "        # compute the similar movies of the \"movie\"        \n",
        "        # movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T, sample_train_sparse_matrix.T).ravel()\n",
        "        movie_sim = 1 - pairwise_distances(sample_train_sparse_matrix[:, movie].T, sample_train_sparse_matrix.T, metric=\"cosine\").ravel()\n",
        "        top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "        # get the ratings of most similar movie rated by this user..\n",
        "        top_ratings = sample_train_sparse_matrix[user, top_sim_movies].toarray().ravel()\n",
        "        # we will make it's length \"5\" by adding user averages to.\n",
        "        top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "        top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
        "    #     print(top_sim_movies_ratings, end=\" : -- \")\n",
        "\n",
        "        #-----------------prepare the row to be stores in a file-----------------#\n",
        "        row = list()\n",
        "        row.append(user)\n",
        "        row.append(movie)\n",
        "        # Now add the other features to this data...\n",
        "        row.append(sample_train_averages['global']) # first feature\n",
        "        # next 5 features are similar_users \"movie\" ratings\n",
        "        row.extend(top_sim_users_ratings)\n",
        "        # next 5 features are \"user\" ratings for similar_movies\n",
        "        row.extend(top_sim_movies_ratings)\n",
        "        # Avg_user rating\n",
        "        row.append(sample_train_averages['user'][user])\n",
        "        # Avg_movie rating\n",
        "        row.append(sample_train_averages['movie'][movie])\n",
        "\n",
        "        # finalley, The actual Rating of this user-movie pair...\n",
        "        row.append(rating)\n",
        "        count = count + 1\n",
        "        index = index + 1\n",
        "\n",
        "        # add rows to the file opened..\n",
        "        reg_data_file.write(','.join(map(str, row)))\n",
        "        reg_data_file.write('\\n')        \n",
        "        if (count)%10000 == 0:\n",
        "            print(index, user, movie)\n",
        "            # print(','.join(map(str, row)))\n",
        "            print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
        "\n",
        "\n",
        "print(datetime.now() - start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us3Wt-iTz3J7"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtHCNfnLVPUb",
        "outputId": "930cf71e-fda4-4b49-c797-ce371190b973"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "preparing 77683 tuples for the dataset..\n",
            "\n",
            "Done for 1000 rows----- 0:03:55.184439\n",
            "Done for 2000 rows----- 0:07:48.998480\n",
            "Done for 3000 rows----- 0:11:42.941655\n",
            "Done for 4000 rows----- 0:15:33.504789\n",
            "Done for 5000 rows----- 0:19:25.174344\n",
            "Done for 6000 rows----- 0:23:15.844064\n",
            "Done for 7000 rows----- 0:27:07.831746\n",
            "Done for 8000 rows----- 0:30:57.836056\n",
            "Done for 9000 rows----- 0:34:51.136102\n",
            "Done for 10000 rows----- 0:38:38.652524\n",
            "Done for 11000 rows----- 0:42:23.888916\n",
            "Done for 12000 rows----- 0:46:09.694808\n",
            "Done for 13000 rows----- 0:49:49.996715\n",
            "Done for 14000 rows----- 0:53:37.661921\n",
            "Done for 15000 rows----- 0:57:16.694860\n",
            "Done for 16000 rows----- 1:01:02.312640\n",
            "Done for 17000 rows----- 1:04:39.587320\n",
            "Done for 18000 rows----- 1:08:23.636830\n",
            "Done for 19000 rows----- 1:12:07.844179\n",
            "Done for 20000 rows----- 1:15:48.269356\n",
            "Done for 21000 rows----- 1:19:26.468834\n",
            "Done for 22000 rows----- 1:23:07.432268\n",
            "Done for 23000 rows----- 1:26:49.926652\n",
            "Done for 24000 rows----- 1:30:30.236362\n",
            "Done for 25000 rows----- 1:34:09.852921\n",
            "Done for 26000 rows----- 1:37:50.204116\n",
            "Done for 27000 rows----- 1:41:31.575893\n",
            "Done for 28000 rows----- 1:45:12.942011\n",
            "Done for 29000 rows----- 1:48:58.091283\n",
            "Done for 30000 rows----- 1:52:40.450362\n",
            "Done for 31000 rows----- 1:56:23.603390\n",
            "Done for 32000 rows----- 2:00:05.011932\n",
            "Done for 33000 rows----- 2:03:45.515303\n",
            "Done for 34000 rows----- 2:07:26.014123\n",
            "Done for 35000 rows----- 2:11:08.683896\n",
            "Done for 36000 rows----- 2:14:48.900920\n",
            "Done for 37000 rows----- 2:18:28.458252\n",
            "Done for 38000 rows----- 2:22:08.982224\n",
            "Done for 39000 rows----- 2:25:50.635971\n",
            "Done for 40000 rows----- 2:29:29.950941\n",
            "Done for 41000 rows----- 2:33:10.736593\n",
            "Done for 42000 rows----- 2:36:54.074568\n",
            "Done for 43000 rows----- 2:40:36.124237\n",
            "Done for 44000 rows----- 2:44:16.843799\n",
            "Done for 45000 rows----- 2:47:58.502462\n",
            "Done for 46000 rows----- 2:51:41.163720\n",
            "Done for 47000 rows----- 2:55:21.135152\n",
            "Done for 48000 rows----- 2:59:06.840113\n",
            "Done for 49000 rows----- 3:02:50.999736\n",
            "Done for 50000 rows----- 3:06:37.477937\n",
            "Done for 51000 rows----- 3:10:24.065887\n",
            "Done for 52000 rows----- 3:14:10.707954\n",
            "Done for 53000 rows----- 3:17:55.998448\n",
            "Done for 54000 rows----- 3:21:42.628662\n",
            "Done for 55000 rows----- 3:25:30.158640\n",
            "Done for 56000 rows----- 3:29:10.208510\n",
            "Done for 57000 rows----- 3:32:58.374282\n",
            "Done for 58000 rows----- 3:36:39.610756\n",
            "Done for 59000 rows----- 3:40:19.027203\n",
            "Done for 60000 rows----- 3:43:59.220430\n",
            "Done for 61000 rows----- 3:47:37.082468\n",
            "Done for 62000 rows----- 3:51:19.521919\n",
            "Done for 63000 rows----- 3:54:58.988793\n",
            "Done for 64000 rows----- 3:58:37.025508\n",
            "Done for 65000 rows----- 4:02:21.577958\n",
            "Done for 66000 rows----- 4:06:05.467202\n",
            "Done for 67000 rows----- 4:09:52.914713\n",
            "Done for 68000 rows----- 4:13:34.918935\n",
            "Done for 69000 rows----- 4:17:19.291674\n",
            "Done for 70000 rows----- 4:20:59.618959\n",
            "Done for 71000 rows----- 4:24:42.727525\n",
            "Done for 72000 rows----- 4:28:28.371266\n",
            "Done for 73000 rows----- 4:32:10.121614\n",
            "Done for 74000 rows----- 4:35:55.999635\n",
            "Done for 75000 rows----- 4:39:42.082919\n",
            "Done for 76000 rows----- 4:43:27.864158\n",
            "Done for 77000 rows----- 4:47:09.118455\n",
            " 4:49:31.050919\n"
          ]
        }
      ],
      "source": [
        "start = datetime.now()\n",
        "\n",
        "if os.path.isfile('reg_test.csv'):\n",
        "    print(\"It is already created...\")\n",
        "else:\n",
        "\n",
        "    print('preparing {} tuples for the dataset..\\n'.format(len(sample_test_ratings)))\n",
        "    with open('reg_test.csv', mode='w') as reg_data_file:\n",
        "        count = 0 \n",
        "        for (user, movie, rating)  in zip(sample_test_users, sample_test_movies, sample_test_ratings):\n",
        "            st = datetime.now()\n",
        "            user, movie, rating = int(user), int(movie), int(rating)\n",
        "        #--------------------- Ratings of \"movie\" by similar users of \"user\" ---------------------\n",
        "            #print(user, movie)\n",
        "            try:\n",
        "                # compute the similar Users of the \"user\"        \n",
        "                user_sim = cosine_similarity(sample_train_sparse_matrix[user].get(), sample_train_sparse_matrix.get()).ravel()\n",
        "                top_sim_users = user_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "                # get the ratings of most similar users for this movie\n",
        "                top_ratings = sample_train_sparse_matrix[top_sim_users, movie].toarray().ravel()\n",
        "                # we will make it's length \"5\" by adding movie averages to .\n",
        "                top_sim_users_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "                top_sim_users_ratings.extend([sample_train_averages['movie'][movie]]*(5 - len(top_sim_users_ratings)))\n",
        "                # print(top_sim_users_ratings, end=\"--\")\n",
        "\n",
        "            except (IndexError, KeyError):\n",
        "                # It is a new User or new Movie or there are no ratings for given user for top similar movies...\n",
        "                ########## Cold STart Problem ##########\n",
        "                top_sim_users_ratings.extend([sample_train_averages['global']]*(5 - len(top_sim_users_ratings)))\n",
        "                #print(top_sim_users_ratings)\n",
        "            except:\n",
        "                print(user, movie)\n",
        "                # we just want KeyErrors to be resolved. Not every Exception...\n",
        "                raise\n",
        "\n",
        "\n",
        "\n",
        "            #--------------------- Ratings by \"user\"  to similar movies of \"movie\" ---------------------\n",
        "            try:\n",
        "                # compute the similar movies of the \"movie\"        \n",
        "                movie_sim = cosine_similarity(sample_train_sparse_matrix[:,movie].T.get(), sample_train_sparse_matrix.T.get()).ravel()\n",
        "                top_sim_movies = movie_sim.argsort()[::-1][1:] # we are ignoring 'The User' from its similar users.\n",
        "                # get the ratings of most similar movie rated by this user..\n",
        "                top_ratings = sample_train_sparse_matrix[user, top_sim_movies].get().toarray().ravel()\n",
        "                # we will make it's length \"5\" by adding user averages to.\n",
        "                top_sim_movies_ratings = list(top_ratings[top_ratings != 0][:5])\n",
        "                top_sim_movies_ratings.extend([sample_train_averages['user'][user]]*(5-len(top_sim_movies_ratings))) \n",
        "                #print(top_sim_movies_ratings)\n",
        "            except (IndexError, KeyError):\n",
        "                #print(top_sim_movies_ratings, end=\" : -- \")\n",
        "                top_sim_movies_ratings.extend([sample_train_averages['global']]*(5-len(top_sim_movies_ratings)))\n",
        "                #print(top_sim_movies_ratings)\n",
        "            except :\n",
        "                raise\n",
        "\n",
        "            #-----------------prepare the row to be stores in a file-----------------#\n",
        "            row = list()\n",
        "            # add usser and movie name first\n",
        "            row.append(user)\n",
        "            row.append(movie)\n",
        "            row.append(sample_train_averages['global']) # first feature\n",
        "            #print(row)\n",
        "            # next 5 features are similar_users \"movie\" ratings\n",
        "            row.extend(top_sim_users_ratings)\n",
        "            #print(row)\n",
        "            # next 5 features are \"user\" ratings for similar_movies\n",
        "            row.extend(top_sim_movies_ratings)\n",
        "            #print(row)\n",
        "            # Avg_user rating\n",
        "            try:\n",
        "                row.append(sample_train_averages['user'][user])\n",
        "            except KeyError:\n",
        "                row.append(sample_train_averages['global'])\n",
        "            except:\n",
        "                raise\n",
        "            #print(row)\n",
        "            # Avg_movie rating\n",
        "            try:\n",
        "                row.append(sample_train_averages['movie'][movie])\n",
        "            except KeyError:\n",
        "                row.append(sample_train_averages['global'])\n",
        "            except:\n",
        "                raise\n",
        "            #print(row)\n",
        "            # finalley, The actual Rating of this user-movie pair...\n",
        "            row.append(rating)\n",
        "            #print(row)\n",
        "            count = count + 1\n",
        "\n",
        "            # add rows to the file opened..\n",
        "            reg_data_file.write(','.join(map(str, row)))\n",
        "            #print(','.join(map(str, row)))\n",
        "            reg_data_file.write('\\n')        \n",
        "            if (count)%1000 == 0:\n",
        "                #print(','.join(map(str, row)))\n",
        "                print(\"Done for {} rows----- {}\".format(count, datetime.now() - start))\n",
        "    print(\"\",datetime.now() - start)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "agLmdd9TahAC",
        "outputId": "8e56a1c2-f379-4a35-d892-973643b2f9bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.7/dist-packages (0.90)\n",
            "Requirement already satisfied: scikit-surprise in /usr/local/lib/python3.7/dist-packages (1.1.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from xgboost) (1.19.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from scikit-surprise) (1.15.0)\n",
            "/content/drive/MyDrive/cosine_similarity_computation\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost scikit-surprise\n",
        "%cd \"/content/drive/MyDrive/cosine_similarity_computation/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7B8G7GAag8m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import xgboost as xgb\n",
        "\n",
        "import random\n",
        "\n",
        "seed = 32\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTAYlGF-3Fh3"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/NicolasHug/surprise.git\n",
        "# %cd surprise\n",
        "reg_train = pd.read_csv('reg_train_30KU.csv')\n",
        "reg_test = pd.read_csv('reg_test_15KU.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8oVToXj9--T",
        "outputId": "72f8987f-3198-48b4-d91f-75ba6f0f527b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of unique users in train 31493\n",
            "# of unique users in test 11012\n"
          ]
        }
      ],
      "source": [
        "print('# of unique users in train', len(np.unique(reg_train['user'])))\n",
        "print('# of unique users in test', len(np.unique(reg_test['user'])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dqw-T_0_5lL",
        "outputId": "049c83e5-e480-4ec3-97d8-6da88ddb2e60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of unique movies in train 6999\n",
            "# of unique movies in test 1279\n"
          ]
        }
      ],
      "source": [
        "print('# of unique movies in train', len(np.unique(reg_train['movie'])))\n",
        "print('# of unique movies in test', len(np.unique(reg_test['movie'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- We can't give raw data (movie, user, rating) to train the model in Surprise library.\n",
        "\n",
        "\n",
        "- They have a saperate format for TRAIN and TEST data, which will be useful for training the models like SVD, KNNBaseLineOnly....etc..,in Surprise.\n",
        "\n",
        "\n",
        "- We can form the trainset from a file, or from a Pandas  DataFrame. \n",
        "http://surprise.readthedocs.io/en/stable/getting_started.html#load-dom-dataframe-py "
      ],
      "metadata": {
        "id": "o_w0g91GjGVZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu6DOy9t4BiI"
      },
      "outputs": [],
      "source": [
        "from surprise import Reader, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXu0AcPj7Vi6"
      },
      "outputs": [],
      "source": [
        "# It is to specify how to read the dataframe.\n",
        "# for our dataframe, we don't have to specify anything extra..\n",
        "reader = Reader(rating_scale=(1,5))\n",
        "\n",
        "# create the traindata from the dataframe...\n",
        "train_data = Dataset.load_from_df(reg_train[['user', 'movie', 'rating']], reader)\n",
        "\n",
        "# build the trainset from traindata.., It is of dataset format from surprise library..\n",
        "trainset = train_data.build_full_trainset() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkzGIQY37XVx"
      },
      "outputs": [],
      "source": [
        "testset = list(zip(reg_test.user.values, reg_test.movie.values, reg_test.rating.values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mhDy1oK8BI-"
      },
      "outputs": [],
      "source": [
        "models_evaluation_train = dict()\n",
        "models_evaluation_test = dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "josKLfA4SDND"
      },
      "outputs": [],
      "source": [
        "# to get rmse and mape given actual and predicted ratings..\n",
        "def get_error_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(np.mean([ (y_true[i] - y_pred[i])**2 for i in range(len(y_pred)) ]))\n",
        "    mape = np.mean(np.abs( (y_true - y_pred)/y_true )) * 100\n",
        "    return rmse, mape\n",
        "\n",
        "###################################################################\n",
        "###################################################################\n",
        "def run_xgboost(algo,  x_train, y_train, x_test, y_test, verbose=True):\n",
        "    \"\"\"\n",
        "    It will return train_results and test_results\n",
        "    \"\"\"\n",
        "    \n",
        "    # dictionaries for storing train and test results\n",
        "    train_results = dict()\n",
        "    test_results = dict()\n",
        "    \n",
        "    \n",
        "    # fit the model\n",
        "    print('Training the model..')\n",
        "    start = datetime.now()\n",
        "    # algo.fit(x_train, y_train, eval_metric = 'rmse')\n",
        "    algo.fit(x_train, y_train)\n",
        "    print('Done. Time taken : {}\\n'.format(datetime.now()-start))\n",
        "    print('Done \\n')\n",
        "\n",
        "    # from the trained model, get the predictions....\n",
        "    print('Evaluating the model with TRAIN data...')\n",
        "    start = datetime.now()\n",
        "    y_train_pred = algo.predict(x_train)\n",
        "    # get the rmse and mape of train data...\n",
        "    rmse_train, mape_train = get_error_metrics(y_train.values, y_train_pred)\n",
        "    \n",
        "    # store the results in train_results dictionary..\n",
        "    train_results = {'rmse': rmse_train,\n",
        "                    'mape' : mape_train,\n",
        "                    'predictions' : y_train_pred}\n",
        "    \n",
        "    #######################################\n",
        "    # get the test data predictions and compute rmse and mape\n",
        "    print('Evaluating Test data')\n",
        "    y_test_pred = algo.predict(x_test) \n",
        "    rmse_test, mape_test = get_error_metrics(y_true=y_test.values, y_pred=y_test_pred)\n",
        "    # store them in our test results dictionary.\n",
        "    test_results = {'rmse': rmse_test,\n",
        "                    'mape' : mape_test,\n",
        "                    'predictions':y_test_pred}\n",
        "    if verbose:\n",
        "        print('\\nTEST DATA')\n",
        "        print('-'*30)\n",
        "        print('RMSE : ', rmse_test)\n",
        "        print('MAPE : ', mape_test)\n",
        "        \n",
        "    # return these train and test results...\n",
        "    return train_results, test_results\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KVHE7PqAwTZ"
      },
      "outputs": [],
      "source": [
        "##########################################################\n",
        "# get  (actual_list , predicted_list) ratings given list \n",
        "# of predictions (prediction is a class in Surprise).    \n",
        "##########################################################\n",
        "def get_ratings(predictions):\n",
        "    actual = np.array([pred.r_ui for pred in predictions])\n",
        "    pred = np.array([pred.est for pred in predictions])\n",
        "    \n",
        "    return actual, pred\n",
        "\n",
        "################################################################\n",
        "# get ''rmse'' and ''mape'' , given list of prediction objecs \n",
        "################################################################\n",
        "def get_errors(predictions, print_them=False):\n",
        "\n",
        "    actual, pred = get_ratings(predictions)\n",
        "    rmse = np.sqrt(np.mean((pred - actual)**2))\n",
        "    mape = np.mean(np.abs(pred - actual)/actual)\n",
        "\n",
        "    return rmse, mape*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w3E-MmNA73P"
      },
      "outputs": [],
      "source": [
        "#################################################################################\n",
        "# It will return predicted ratings, rmse and mape of both train and test data   #\n",
        "##################################################################################\n",
        "def run_surprise(algo, trainset, testset, verbose=True): \n",
        "    '''\n",
        "        return train_dict, test_dict\n",
        "    \n",
        "        It returns two dictionaries, one for train and the other is for test\n",
        "        Each of them have 3 key-value pairs, which specify ''rmse'', ''mape'', and ''predicted ratings''.\n",
        "    '''\n",
        "    start = datetime.now()\n",
        "    # dictionaries that stores metrics for train and test..\n",
        "    train = dict()\n",
        "    test = dict()\n",
        "    \n",
        "    # train the algorithm with the trainset\n",
        "    st = datetime.now()\n",
        "    print('Training the model...')\n",
        "    algo.fit(trainset)\n",
        "    print('Done. time taken : {} \\n'.format(datetime.now()-st))\n",
        "    \n",
        "    # ---------------- Evaluating train data--------------------#\n",
        "    st = datetime.now()\n",
        "    print('Evaluating the model with train data..')\n",
        "    # get the train predictions (list of prediction class inside Surprise)\n",
        "    train_preds = algo.test(trainset.build_testset())\n",
        "    # get predicted ratings from the train predictions..\n",
        "    train_actual_ratings, train_pred_ratings = get_ratings(train_preds)\n",
        "    # get ''rmse'' and ''mape'' from the train predictions.\n",
        "    train_rmse, train_mape = get_errors(train_preds)\n",
        "    print('time taken : {}'.format(datetime.now()-st))\n",
        "    \n",
        "    if verbose:\n",
        "        print('-'*15)\n",
        "        print('Train Data')\n",
        "        print('-'*15)\n",
        "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(train_rmse, train_mape))\n",
        "    \n",
        "    #store them in the train dictionary\n",
        "    if verbose:\n",
        "        print('adding train results in the dictionary..')\n",
        "    train['rmse'] = train_rmse\n",
        "    train['mape'] = train_mape\n",
        "    train['predictions'] = train_pred_ratings\n",
        "    \n",
        "    #------------ Evaluating Test data---------------#\n",
        "    st = datetime.now()\n",
        "    print('\\nEvaluating for test data...')\n",
        "    # get the predictions( list of prediction classes) of test data\n",
        "    test_preds = algo.test(testset)\n",
        "    # get the predicted ratings from the list of predictions\n",
        "    test_actual_ratings, test_pred_ratings = get_ratings(test_preds)\n",
        "    # get error metrics from the predicted and actual ratings\n",
        "    test_rmse, test_mape = get_errors(test_preds)\n",
        "    print('time taken : {}'.format(datetime.now()-st))\n",
        "    \n",
        "    if verbose:\n",
        "        print('-'*15)\n",
        "        print('Test Data')\n",
        "        print('-'*15)\n",
        "        print(\"RMSE : {}\\n\\nMAPE : {}\\n\".format(test_rmse, test_mape))\n",
        "    # store them in test dictionary\n",
        "    if verbose:\n",
        "        print('storing the test results in test dictionary...')\n",
        "    test['rmse'] = test_rmse\n",
        "    test['mape'] = test_mape\n",
        "    test['predictions'] = test_pred_ratings\n",
        "    \n",
        "    print('\\n'+'-'*45)\n",
        "    print('Total time taken to run this algorithm :', datetime.now() - start)\n",
        "    \n",
        "    # return two dictionaries train and test\n",
        "    return train, test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du2qi1n_BHCv",
        "outputId": "f1e1214d-c275-49e4-e69f-f4e4a0ab1e92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using matplotlib backend: agg\n",
            "Training the model..\n",
            "Done. Time taken : 0:00:34.528700\n",
            "\n",
            "Done \n",
            "\n",
            "Evaluating the model with TRAIN data...\n",
            "Evaluating Test data\n",
            "\n",
            "TEST DATA\n",
            "------------------------------\n",
            "RMSE :  1.0760162324179263\n",
            "MAPE :  32.763482502383305\n"
          ]
        }
      ],
      "source": [
        "%matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
        "from sklearn.kernel_approximation import Nystroem, AdditiveChi2Sampler\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from datetime import datetime\n",
        "\n",
        "# prepare Train data\n",
        "x_train = reg_train.drop(['user','movie','rating'], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# Prepare Test data\n",
        "x_test = reg_test.drop(['user','movie','rating'], axis=1)\n",
        "y_test = reg_test['rating']\n",
        "\n",
        "# initialize Our first XGBoost model...\n",
        "# first_xgb = xgb.XGBRegressor(silent=False, n_jobs=13, random_state=200, n_estimators=15, gpu_id=0, tree_method='gpu_hist', reg_lambda=1, \n",
        "#                              sampling_method=\"gradient_based\", max_bin=4096, learning_rate=0.3,\n",
        "#                              max_depth=15, booster='dart')\n",
        "# first_xgb = SGDRegressor(loss=\"huber\", learning_rate=\"adaptive\", penalty=\"l2\")\n",
        "# kernel_transformer = AdditiveChi2Sampler()\n",
        "\n",
        "# x_train = kernel_transformer.fit_transform(x_train)\n",
        "# x_test = kernel_transformer.transform(x_test)\n",
        "\n",
        "first_xgb = SGDRegressor(loss=\"huber\", learning_rate=\"adaptive\", penalty=\"l2\")\n",
        "train_results, test_results = run_xgboost(first_xgb, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['first_algo'] = train_results\n",
        "models_evaluation_test['first_algo'] = test_results\n",
        "\n",
        "# xgb.plot_importance(first_xgb)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Predicted_rating : ( baseline prediction ) __\n",
        "\n",
        "    -  http://surprise.readthedocs.io/en/stable/basic_algorithms.html#surprise.prediction_algorithms.baseline_only.BaselineOnly \n",
        " >$   \\large {\\hat{r}_{ui} = b_{ui} =\\mu + b_u + b_i} $\n",
        "\n",
        "\n",
        "- $\\pmb \\mu $ : Average of all trainings in training data.\n",
        "- $\\pmb b_u$ : User bias\n",
        "- $\\pmb b_i$ : Item bias (movie biases) "
      ],
      "metadata": {
        "id": "GfliXrLtjU7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "__Optimization function ( Least Squares Problem ) __\n",
        "\n",
        "    - http://surprise.readthedocs.io/en/stable/prediction_algorithms.html#baselines-estimates-configuration \n",
        "\n",
        "> $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - (\\mu + b_u + b_i)\\right)^2 +\n",
        "\\lambda \\left(b_u^2 + b_i^2 \\right).\\text {        [mimimize } {b_u, b_i]}$ "
      ],
      "metadata": {
        "id": "bBWu8q2ejijE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TrhabE1CByS6"
      },
      "outputs": [],
      "source": [
        "from surprise import BaselineOnly "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP7JzLn_EVPS",
        "outputId": "01caf107-7b64-47e2-cc3f-948209bf75e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Estimating biases using sgd...\n",
            "Done. time taken : 0:00:04.838103 \n",
            "\n",
            "Evaluating the model with train data..\n",
            "time taken : 0:00:15.033884\n",
            "---------------\n",
            "Train Data\n",
            "---------------\n",
            "RMSE : 3267.872247633649\n",
            "\n",
            "MAPE : 46.3947695136529\n",
            "\n",
            "adding train results in the dictionary..\n",
            "\n",
            "Evaluating for test data...\n",
            "time taken : 0:00:00.688662\n",
            "---------------\n",
            "Test Data\n",
            "---------------\n",
            "RMSE : 1.5165482049240204\n",
            "\n",
            "MAPE : 47.90767652723992\n",
            "\n",
            "storing the test results in test dictionary...\n",
            "\n",
            "---------------------------------------------\n",
            "Total time taken to run this algorithm : 0:00:20.561378\n"
          ]
        }
      ],
      "source": [
        "# options are to specify.., how to compute those user and item biases\n",
        "bsl_options = {'method': 'sgd',\n",
        "               'learning_rate': 0.01\n",
        "               }\n",
        "bsl_algo = BaselineOnly(bsl_options=bsl_options)\n",
        "# run this algorithm.., It will return the train and test results..\n",
        "bsl_train_results, bsl_test_results = run_surprise(bsl_algo, trainset, testset, verbose=True)\n",
        "\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['bsl_algo'] = bsl_train_results\n",
        "models_evaluation_test['bsl_algo'] = bsl_test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "jzL2EOvdEYOA",
        "outputId": "0895b5f2-e607-4c4e-f869-ddd725b5216a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>movie</th>\n",
              "      <th>GAvg</th>\n",
              "      <th>sur1</th>\n",
              "      <th>sur2</th>\n",
              "      <th>sur3</th>\n",
              "      <th>sur4</th>\n",
              "      <th>sur5</th>\n",
              "      <th>smr1</th>\n",
              "      <th>smr2</th>\n",
              "      <th>smr3</th>\n",
              "      <th>smr4</th>\n",
              "      <th>smr5</th>\n",
              "      <th>UAvg</th>\n",
              "      <th>MAvg</th>\n",
              "      <th>rating</th>\n",
              "      <th>bslpr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>30</td>\n",
              "      <td>3.585633</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.007812</td>\n",
              "      <td>3.790133</td>\n",
              "      <td>5</td>\n",
              "      <td>4.329560</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>185</td>\n",
              "      <td>3.585633</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.007812</td>\n",
              "      <td>2.944099</td>\n",
              "      <td>4</td>\n",
              "      <td>3.630428</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user  movie      GAvg  sur1  ...      UAvg      MAvg  rating     bslpr\n",
              "0     7     30  3.585633   4.0  ...  4.007812  3.790133       5  4.329560\n",
              "1     7    185  3.585633   4.0  ...  4.007812  2.944099       4  3.630428\n",
              "\n",
              "[2 rows x 17 columns]"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add our baseline_predicted value as our feature..\n",
        "reg_train['bslpr'] = models_evaluation_train['bsl_algo']['predictions']\n",
        "reg_train.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "K4O5IXu0Fpgc",
        "outputId": "1df1cf54-bf81-4fd4-829c-e44887aae8e5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>movie</th>\n",
              "      <th>GAvg</th>\n",
              "      <th>sur1</th>\n",
              "      <th>sur2</th>\n",
              "      <th>sur3</th>\n",
              "      <th>sur4</th>\n",
              "      <th>sur5</th>\n",
              "      <th>smr1</th>\n",
              "      <th>smr2</th>\n",
              "      <th>smr3</th>\n",
              "      <th>smr4</th>\n",
              "      <th>smr5</th>\n",
              "      <th>UAvg</th>\n",
              "      <th>MAvg</th>\n",
              "      <th>rating</th>\n",
              "      <th>bslpr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>3936</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>6386</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user  movie      GAvg      sur1  ...      UAvg      MAvg  rating  bslpr\n",
              "0     8   3936  3.578906  3.578906  ...  3.578906  3.578906       4    5.0\n",
              "1     8   6386  3.578906  3.578906  ...  3.578906  3.578906       5    5.0\n",
              "\n",
              "[2 rows x 17 columns]"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_test['bslpr']  = models_evaluation_test['bsl_algo']['predictions']\n",
        "reg_test.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fick_c8TEiLz",
        "outputId": "4e10a8cc-308e-4aaf-87fe-3948e4833410"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model..\n",
            "Done. Time taken : 0:00:31.812145\n",
            "\n",
            "Done \n",
            "\n",
            "Evaluating the model with TRAIN data...\n",
            "Evaluating Test data\n",
            "\n",
            "TEST DATA\n",
            "------------------------------\n",
            "RMSE :  1.0767756364568846\n",
            "MAPE :  32.728655803765854\n"
          ]
        }
      ],
      "source": [
        "# prepare train data\n",
        "x_train = reg_train.drop(['user', 'movie','rating'], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# Prepare Test data\n",
        "x_test = reg_test.drop(['user','movie','rating'], axis=1)\n",
        "y_test = reg_test['rating']\n",
        "\n",
        "# initialize Our first XGBoost model...\n",
        "# xgb_bsl = xgb.XGBRegressor(silent=False, n_jobs=13, random_state=15, n_estimators=100, gpu_id=0, tree_method='gpu_hist')\n",
        "xgb_bsl = SGDRegressor(loss=\"huber\", learning_rate=\"adaptive\", penalty=\"elasticnet\")\n",
        "train_results, test_results = run_xgboost(xgb_bsl, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_bsl'] = train_results\n",
        "models_evaluation_test['xgb_bsl'] = test_results\n",
        "\n",
        "# xgb.plot_importance(xgb_bsl)\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- KNN BASELINE\n",
        "    - http://surprise.readthedocs.io/en/stable/knn_inspired.html#surprise.prediction_algorithms.knns.KNNBaseline "
      ],
      "metadata": {
        "id": "mLhfn_LWjptF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeiLXXC2FLpw"
      },
      "outputs": [],
      "source": [
        "from surprise import KNNBaseline, KNNWithZScore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Trjr2AgQHXYb",
        "outputId": "b08b3160-8f0e-4430-8090-c9e542a68b70"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Estimating biases using sgd...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Done. time taken : 0:21:19.868142 \n",
            "\n",
            "Evaluating the model with train data..\n",
            "time taken : 2:15:09.648990\n",
            "---------------\n",
            "Train Data\n",
            "---------------\n",
            "RMSE : 3267.8717881322973\n",
            "\n",
            "MAPE : 23.11124309680427\n",
            "\n",
            "adding train results in the dictionary..\n",
            "\n",
            "Evaluating for test data...\n",
            "time taken : 0:00:06.310134\n",
            "---------------\n",
            "Test Data\n",
            "---------------\n",
            "RMSE : 1.6644940383654292\n",
            "\n",
            "MAPE : 54.62470383960838\n",
            "\n",
            "storing the test results in test dictionary...\n",
            "\n",
            "---------------------------------------------\n",
            "Total time taken to run this algorithm : 2:36:35.829399\n"
          ]
        }
      ],
      "source": [
        "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
        "sim_options = {'user_based' : True,\n",
        "               'name': 'pearson_baseline',\n",
        "               'shrinkage': 100,\n",
        "               'min_support': 2\n",
        "              } \n",
        "# we keep other parameters like regularization parameter and learning_rate as default values.\n",
        "bsl_options = {'method': 'sgd'} \n",
        "\n",
        "knn_bsl_u = KNNWithZScore(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
        "knn_bsl_u_train_results, knn_bsl_u_test_results = run_surprise(knn_bsl_u, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['knn_bsl_u'] = knn_bsl_u_train_results \n",
        "models_evaluation_test['knn_bsl_u'] = knn_bsl_u_test_results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- PEARSON_BASELINE SIMILARITY\n",
        "    - http://surprise.readthedocs.io/en/stable/similarities.html#surprise.similarities.pearson_baseline "
      ],
      "metadata": {
        "id": "KAEJGxlkkquq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- SHRINKAGE\n",
        "    - _2.2 Neighborhood Models_ in http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf "
      ],
      "metadata": {
        "id": "CoVZ3MbRk5wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __predicted Rating__ : ( ___ based on User-User similarity ___ )\n",
        "\n",
        "\\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
        "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
        "N^k_i(u)} \\text{sim}(u, v)} \\end{align}\n",
        "\n",
        "- $\\pmb{b_{ui}}$ -  _Baseline prediction_ of (user,movie) rating\n",
        "\n",
        "- $ \\pmb {N_i^k (u)}$ - Set of __K similar__ users (neighbours) of __user (u)__ who rated __movie(i)__  \n",
        "\n",
        "- _sim (u, v)_ - __Similarity__ between users __u and v__  \n",
        "    - Generally, it will be cosine similarity or Pearson correlation coefficient. \n",
        "    - But we use __shrunk Pearson-baseline correlation coefficient__, which is based on the pearsonBaseline similarity ( we take base line predictions instead of mean rating of user/item)\n",
        "       "
      ],
      "metadata": {
        "id": "CkJRWJbtk8Yy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __ Predicted rating __ ( based on Item Item similarity ):\n",
        " \\begin{align} \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in N^k_u(j)} \\text{sim}(i, j)} \\end{align}\n",
        "\n",
        "    -  ___Notations follows same as above (user user based predicted rating ) ___"
      ],
      "metadata": {
        "id": "CUvzxy9Xk_9L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ZT8AxAZ0Sc7H",
        "outputId": "0c529769-951b-44f7-ba44-4da991105c48"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Estimating biases using sgd...\n",
            "Computing the pearson_baseline similarity matrix...\n",
            "Done computing similarity matrix.\n",
            "Done. time taken : 0:00:30.237522 \n",
            "\n",
            "Evaluating the model with train data..\n",
            "time taken : 0:06:20.722268\n",
            "---------------\n",
            "Train Data\n",
            "---------------\n",
            "RMSE : 3267.87180605701\n",
            "\n",
            "MAPE : 25.610201860816506\n",
            "\n",
            "adding train results in the dictionary..\n",
            "\n",
            "Evaluating for test data...\n",
            "time taken : 0:00:00.893109\n",
            "---------------\n",
            "Test Data\n",
            "---------------\n",
            "RMSE : 1.663634521380457\n",
            "\n",
            "MAPE : 54.588593609838576\n",
            "\n",
            "storing the test results in test dictionary...\n",
            "\n",
            "---------------------------------------------\n",
            "Total time taken to run this algorithm : 0:06:51.855800\n"
          ]
        }
      ],
      "source": [
        "# we specify , how to compute similarities and what to consider with sim_options to our algorithm\n",
        "\n",
        "# 'user_based' : Fals => this considers the similarities of movies instead of users\n",
        "\n",
        "sim_options = {'user_based' : False,\n",
        "               'name': 'pearson_baseline',\n",
        "               'shrinkage': 100,\n",
        "               'min_support': 2\n",
        "              } \n",
        "# we keep other parameters like regularization parameter and learning_rate as default values.\n",
        "bsl_options = {'method': 'sgd'}\n",
        "\n",
        "\n",
        "knn_bsl_m = KNNWithZScore(k=40, sim_options = sim_options, bsl_options = bsl_options)\n",
        "\n",
        "knn_bsl_m_train_results, knn_bsl_m_test_results = run_surprise(knn_bsl_m, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['knn_bsl_m'] = knn_bsl_m_train_results \n",
        "models_evaluation_test['knn_bsl_m'] = knn_bsl_m_test_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "VXCppvh1Hyws",
        "outputId": "7f52b5ca-4b17-486a-c263-315bbc1d828b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>movie</th>\n",
              "      <th>GAvg</th>\n",
              "      <th>sur1</th>\n",
              "      <th>sur2</th>\n",
              "      <th>sur3</th>\n",
              "      <th>sur4</th>\n",
              "      <th>sur5</th>\n",
              "      <th>smr1</th>\n",
              "      <th>smr2</th>\n",
              "      <th>smr3</th>\n",
              "      <th>smr4</th>\n",
              "      <th>smr5</th>\n",
              "      <th>UAvg</th>\n",
              "      <th>MAvg</th>\n",
              "      <th>rating</th>\n",
              "      <th>bslpr</th>\n",
              "      <th>knn_bsl_u</th>\n",
              "      <th>knn_bsl_m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>30</td>\n",
              "      <td>3.585633</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.007812</td>\n",
              "      <td>3.790133</td>\n",
              "      <td>5</td>\n",
              "      <td>4.329560</td>\n",
              "      <td>4.301667</td>\n",
              "      <td>4.309889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>185</td>\n",
              "      <td>3.585633</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.007812</td>\n",
              "      <td>2.944099</td>\n",
              "      <td>4</td>\n",
              "      <td>3.630428</td>\n",
              "      <td>3.773317</td>\n",
              "      <td>3.637829</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user  movie      GAvg  sur1  ...  rating     bslpr  knn_bsl_u  knn_bsl_m\n",
              "0     7     30  3.585633   4.0  ...       5  4.329560   4.301667   4.309889\n",
              "1     7    185  3.585633   4.0  ...       4  3.630428   3.773317   3.637829\n",
              "\n",
              "[2 rows x 19 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add the predicted values from both knns to this dataframe\n",
        "reg_train['knn_bsl_u'] = models_evaluation_train['knn_bsl_u']['predictions']\n",
        "reg_train['knn_bsl_m'] = models_evaluation_train['knn_bsl_m']['predictions']\n",
        "reg_train.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s_KSqH7qH1k4",
        "outputId": "5a7ed409-3e18-4b3a-bb93-cf21002e331f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>movie</th>\n",
              "      <th>GAvg</th>\n",
              "      <th>sur1</th>\n",
              "      <th>sur2</th>\n",
              "      <th>sur3</th>\n",
              "      <th>sur4</th>\n",
              "      <th>sur5</th>\n",
              "      <th>smr1</th>\n",
              "      <th>smr2</th>\n",
              "      <th>smr3</th>\n",
              "      <th>smr4</th>\n",
              "      <th>smr5</th>\n",
              "      <th>UAvg</th>\n",
              "      <th>MAvg</th>\n",
              "      <th>rating</th>\n",
              "      <th>bslpr</th>\n",
              "      <th>knn_bsl_u</th>\n",
              "      <th>knn_bsl_m</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>3936</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>6386</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user  movie      GAvg      sur1  ...  rating  bslpr  knn_bsl_u  knn_bsl_m\n",
              "0     8   3936  3.578906  3.578906  ...       4    5.0        5.0        5.0\n",
              "1     8   6386  3.578906  3.578906  ...       5    5.0        5.0        5.0\n",
              "\n",
              "[2 rows x 19 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_test['knn_bsl_u'] = models_evaluation_test['knn_bsl_u']['predictions']\n",
        "reg_test['knn_bsl_m'] = models_evaluation_test['knn_bsl_m']['predictions']\n",
        "\n",
        "reg_test.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cDSY4zSdH3XT",
        "outputId": "fbfba3b5-8c85-4773-8ef5-523fb37ac7c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model..\n",
            "Done. Time taken : 0:00:32.098960\n",
            "\n",
            "Done \n",
            "\n",
            "Evaluating the model with TRAIN data...\n",
            "Evaluating Test data\n",
            "\n",
            "TEST DATA\n",
            "------------------------------\n",
            "RMSE :  1.0706272280968663\n",
            "MAPE :  32.925854476335005\n"
          ]
        }
      ],
      "source": [
        "# prepare the train data....\n",
        "x_train = reg_train.drop(['user', 'movie', 'rating'], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# prepare the train data....\n",
        "x_test = reg_test.drop(['user','movie','rating'], axis=1)\n",
        "y_test = reg_test['rating']\n",
        "\n",
        "# declare the model\n",
        "# xgb_knn_bsl = xgb.XGBRegressor(n_jobs=10, random_state=15, gpu_id=0, tree_method='gpu_hist')\n",
        "xgb_knn_bsl = SGDRegressor(loss=\"huber\", learning_rate=\"adaptive\", penalty=\"l2\")\n",
        "train_results, test_results = run_xgboost(xgb_knn_bsl, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_knn_bsl'] = train_results\n",
        "models_evaluation_test['xgb_knn_bsl'] = test_results\n",
        "\n",
        "\n",
        "# xgb.plot_importance(xgb_knn_bsl)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "http://surprise.readthedocs.io/en/stable/matrix_factorization.html#surprise.prediction_algorithms.matrix_factorization.SVD "
      ],
      "metadata": {
        "id": "pMAMEC-ikTIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __ Predicted Rating : __\n",
        "    - \n",
        "    - $ \\large  \\hat r_{ui} = \\mu + b_u + b_i + q_i^Tp_u $\n",
        "    \n",
        "        - $\\pmb q_i$ - Representation of item(movie) in latent factor space\n",
        "        \n",
        "        - $\\pmb p_u$ - Representation of user in new latent factor space\n",
        "        \n"
      ],
      "metadata": {
        "id": "7GkwAwNmkVtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A BASIC MATRIX FACTORIZATION MODEL in  https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf"
      ],
      "metadata": {
        "id": "Nh_yOCwGkYlZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
        "    - \n",
        "    - $\\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
        "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right) $"
      ],
      "metadata": {
        "id": "lYI2cC3GkbtJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BvaCd3kOH5TL"
      },
      "outputs": [],
      "source": [
        "from surprise import SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XKCOEr1iH66h",
        "outputId": "a51f6a71-0460-461f-aa11-10ca828cff7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            "Processing epoch 0\n",
            "Processing epoch 1\n",
            "Processing epoch 2\n",
            "Processing epoch 3\n",
            "Processing epoch 4\n",
            "Processing epoch 5\n",
            "Processing epoch 6\n",
            "Processing epoch 7\n",
            "Processing epoch 8\n",
            "Processing epoch 9\n",
            "Processing epoch 10\n",
            "Processing epoch 11\n",
            "Processing epoch 12\n",
            "Processing epoch 13\n",
            "Processing epoch 14\n",
            "Processing epoch 15\n",
            "Processing epoch 16\n",
            "Processing epoch 17\n",
            "Processing epoch 18\n",
            "Processing epoch 19\n",
            "Done. time taken : 0:01:35.573361 \n",
            "\n",
            "Evaluating the model with train data..\n",
            "time taken : 0:00:19.522100\n",
            "---------------\n",
            "Train Data\n",
            "---------------\n",
            "RMSE : 3267.8721799055943\n",
            "\n",
            "MAPE : 61.85151835050975\n",
            "\n",
            "adding train results in the dictionary..\n",
            "\n",
            "Evaluating for test data...\n",
            "time taken : 0:00:01.312970\n",
            "---------------\n",
            "Test Data\n",
            "---------------\n",
            "RMSE : 1.6777399910041546\n",
            "\n",
            "MAPE : 55.25812168256461\n",
            "\n",
            "storing the test results in test dictionary...\n",
            "\n",
            "---------------------------------------------\n",
            "Total time taken to run this algorithm : 0:01:56.411442\n"
          ]
        }
      ],
      "source": [
        "# initiallize the model\n",
        "svd = SVD(n_factors=100, biased=True, random_state=15, verbose=True)\n",
        "svd_train_results, svd_test_results = run_surprise(svd, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['svd'] = svd_train_results \n",
        "models_evaluation_test['svd'] = svd_test_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ----->  2.5 Implicit Feedback in http://courses.ischool.berkeley.edu/i290-dm/s11/SECURE/a1-koren.pdf"
      ],
      "metadata": {
        "id": "A8EaB8TSj3j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __ Predicted Rating : __\n",
        "    - $ \\large \\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
        "    |I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right) $ "
      ],
      "metadata": {
        "id": "EeDqLUwOj7Ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - $ \\pmb{I_u}$ --- the set of all items rated by user u\n",
        "\n",
        "- $\\pmb{y_j}$ --- Our new set of item factors that capture implicit ratings.  "
      ],
      "metadata": {
        "id": "sx9vsTvRkBdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
        "    - \n",
        "    - $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
        "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2 + ||y_j||^2\\right) $ "
      ],
      "metadata": {
        "id": "QsFIBnjVkEo-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wSunq-geH8pe"
      },
      "outputs": [],
      "source": [
        "from surprise import SVDpp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMNlz597H-Dj",
        "outputId": "a4fe94d5-e3a8-497b-c5b2-26c7418c0c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training the model...\n",
            " processing epoch 0\n",
            " processing epoch 1\n",
            " processing epoch 2\n",
            " processing epoch 3\n",
            " processing epoch 4\n",
            " processing epoch 5\n",
            " processing epoch 6\n",
            " processing epoch 7\n",
            " processing epoch 8\n",
            " processing epoch 9\n",
            " processing epoch 10\n",
            " processing epoch 11\n",
            " processing epoch 12\n",
            " processing epoch 13\n",
            " processing epoch 14\n",
            " processing epoch 15\n",
            " processing epoch 16\n",
            " processing epoch 17\n",
            " processing epoch 18\n",
            " processing epoch 19\n",
            "Done. time taken : 2:13:06.618435 \n",
            "\n",
            "Evaluating the model with train data..\n",
            "time taken : 0:05:30.395810\n",
            "---------------\n",
            "Train Data\n",
            "---------------\n",
            "RMSE : 3267.8721799055943\n",
            "\n",
            "MAPE : 61.85151835050975\n",
            "\n",
            "adding train results in the dictionary..\n",
            "\n",
            "Evaluating for test data...\n",
            "time taken : 0:00:01.521087\n",
            "---------------\n",
            "Test Data\n",
            "---------------\n",
            "RMSE : 1.6777399910041546\n",
            "\n",
            "MAPE : 55.25812168256461\n",
            "\n",
            "storing the test results in test dictionary...\n",
            "\n",
            "---------------------------------------------\n",
            "Total time taken to run this algorithm : 2:18:38.537986\n"
          ]
        }
      ],
      "source": [
        "# initiallize the model\n",
        "svdpp = SVDpp(n_factors=50, random_state=15, verbose=True)\n",
        "svdpp_train_results, svdpp_test_results = run_surprise(svdpp, trainset, testset, verbose=True)\n",
        "\n",
        "# Just store these error metrics in our models_evaluation datastructure\n",
        "models_evaluation_train['svdpp'] = svdpp_train_results \n",
        "models_evaluation_test['svdpp'] = svdpp_test_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fVkTWF9LIAK9",
        "outputId": "830e9ad5-3c2b-4a56-e1a2-17f3f4aaea2c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>movie</th>\n",
              "      <th>GAvg</th>\n",
              "      <th>sur1</th>\n",
              "      <th>sur2</th>\n",
              "      <th>sur3</th>\n",
              "      <th>sur4</th>\n",
              "      <th>sur5</th>\n",
              "      <th>smr1</th>\n",
              "      <th>smr2</th>\n",
              "      <th>smr3</th>\n",
              "      <th>smr4</th>\n",
              "      <th>smr5</th>\n",
              "      <th>UAvg</th>\n",
              "      <th>MAvg</th>\n",
              "      <th>rating</th>\n",
              "      <th>bslpr</th>\n",
              "      <th>knn_bsl_u</th>\n",
              "      <th>knn_bsl_m</th>\n",
              "      <th>svd</th>\n",
              "      <th>svdpp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>30</td>\n",
              "      <td>3.585633</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.007812</td>\n",
              "      <td>3.790133</td>\n",
              "      <td>5</td>\n",
              "      <td>4.329560</td>\n",
              "      <td>4.301667</td>\n",
              "      <td>4.309889</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7</td>\n",
              "      <td>185</td>\n",
              "      <td>3.585633</td>\n",
              "      <td>4.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>4.007812</td>\n",
              "      <td>2.944099</td>\n",
              "      <td>4</td>\n",
              "      <td>3.630428</td>\n",
              "      <td>3.773317</td>\n",
              "      <td>3.637829</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user  movie      GAvg  sur1  ...  knn_bsl_u  knn_bsl_m  svd  svdpp\n",
              "0     7     30  3.585633   4.0  ...   4.301667   4.309889    5      5\n",
              "1     7    185  3.585633   4.0  ...   3.773317   3.637829    5      5\n",
              "\n",
              "[2 rows x 21 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# add the predicted values from both knns to this dataframe\n",
        "reg_train['svd'] = models_evaluation_train['svd']['predictions']\n",
        "reg_train['svdpp'] = models_evaluation_train['svdpp']['predictions']\n",
        "\n",
        "reg_train.head(2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "d7JEAPLyIB6S",
        "outputId": "32bb6ad1-06a4-4f96-bee6-b9f8dd515ab0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>user</th>\n",
              "      <th>movie</th>\n",
              "      <th>GAvg</th>\n",
              "      <th>sur1</th>\n",
              "      <th>sur2</th>\n",
              "      <th>sur3</th>\n",
              "      <th>sur4</th>\n",
              "      <th>sur5</th>\n",
              "      <th>smr1</th>\n",
              "      <th>smr2</th>\n",
              "      <th>smr3</th>\n",
              "      <th>smr4</th>\n",
              "      <th>smr5</th>\n",
              "      <th>UAvg</th>\n",
              "      <th>MAvg</th>\n",
              "      <th>rating</th>\n",
              "      <th>bslpr</th>\n",
              "      <th>knn_bsl_u</th>\n",
              "      <th>knn_bsl_m</th>\n",
              "      <th>svd</th>\n",
              "      <th>svdpp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8</td>\n",
              "      <td>3936</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>4</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8</td>\n",
              "      <td>6386</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>3.578906</td>\n",
              "      <td>5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   user  movie      GAvg      sur1  ...  knn_bsl_u  knn_bsl_m  svd  svdpp\n",
              "0     8   3936  3.578906  3.578906  ...        5.0        5.0    5      5\n",
              "1     8   6386  3.578906  3.578906  ...        5.0        5.0    5      5\n",
              "\n",
              "[2 rows x 21 columns]"
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "reg_test['svd'] = models_evaluation_test['svd']['predictions']\n",
        "reg_test['svdpp'] = models_evaluation_test['svdpp']['predictions']\n",
        "\n",
        "reg_test.head(2) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr22WOKsIDq9"
      },
      "outputs": [],
      "source": [
        "# prepare x_train and y_train\n",
        "x_train = reg_train.drop(['user', 'movie', 'rating',], axis=1)\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# prepare test data\n",
        "x_test = reg_test.drop(['user', 'movie', 'rating'], axis=1)\n",
        "y_test = reg_test['rating']\n",
        "\n",
        "\n",
        "# xgb_final = xgb.XGBRegressor(n_jobs=10, random_state=15, gpu_id=0)\n",
        "xgb_final = SGDRegressor(loss=\"huber\", learning_rate=\"adaptive\", penalty=\"l2\")\n",
        "train_results, test_results = run_xgboost(xgb_final, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_final'] = train_results\n",
        "models_evaluation_test['xgb_final'] = test_results\n",
        "\n",
        "\n",
        "# xgb.plot_importance(xgb_final)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c50Am1p2IKfg"
      },
      "outputs": [],
      "source": [
        "# prepare train data\n",
        "x_train = reg_train[['knn_bsl_u', 'knn_bsl_m', 'svd', 'svdpp']]\n",
        "y_train = reg_train['rating']\n",
        "\n",
        "# test data\n",
        "x_test = reg_test_df[['knn_bsl_u', 'knn_bsl_m', 'svd', 'svdpp']]\n",
        "y_test = reg_test_df['rating']\n",
        "\n",
        "\n",
        "# xgb_all_models = xgb.XGBRegressor(n_jobs=10, random_state=15, gpu_id=0)\n",
        "xgb_all_models = SGDRegressor(loss=\"huber\", learning_rate=\"adaptive\", penalty=\"l2\")\n",
        "train_results, test_results = run_xgboost(xgb_all_models, x_train, y_train, x_test, y_test)\n",
        "\n",
        "# store the results in models_evaluations dictionaries\n",
        "models_evaluation_train['xgb_all_models'] = train_results\n",
        "models_evaluation_test['xgb_all_models'] = test_results\n",
        "\n",
        "# xgb.plot_importance(xgb_all_models)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wz5l2WoIOFM"
      },
      "outputs": [],
      "source": [
        "# Saving our TEST_RESULTS into a dataframe so that you don't have to run it again\n",
        "pd.DataFrame(models_evaluation_test).to_csv('sample_results.csv')\n",
        "models = pd.read_csv('sample_results.csv', index_col=0)\n",
        "models.loc['rmse'].sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLP-mo1FxECW"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "cosine_similarity.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}